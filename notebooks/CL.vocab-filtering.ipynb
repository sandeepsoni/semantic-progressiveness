{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning the embeddings, obtain a _common_ vocabulary. This vocabulary should:\n",
    "\n",
    "- be common to both the early documents as well as the late documents.\n",
    "- should not contain any names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input embeddings are ```model.wv.vectors``` and the outout vectors are ```model.trainables.syn1neg```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "import gensim\n",
    "import sys\n",
    "if \"../\" not in sys.path: sys.path.append (\"../\")\n",
    "\n",
    "from modules.propernames import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7cd9d38690>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"/hg191/corpora/legaldata/models\"\n",
    "STATS_DIR = \"/hg191/corpora/legaldata/data/stats\"\n",
    "EARLY_FILE = os.path.join (MODELS_DIR, \"sgns.500K.early.100.model\")\n",
    "LATER_FILE = os.path.join (MODELS_DIR, \"sgns.500K.later.100.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_model = gensim.models.Word2Vec.load(EARLY_FILE)\n",
    "later_model = gensim.models.Word2Vec.load(LATER_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_vocab = {key for key in early_model.wv.vocab.keys()}\n",
    "later_vocab = {key for key in later_model.wv.vocab.keys()}\n",
    "common_vocab = set.intersection (early_vocab, later_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readExamples (filename):\n",
    "    with open (filename) as fin:\n",
    "        sample = [line.strip() for line in fin]\n",
    "    positives = [candidate.strip(\"+\") for candidate in sample if candidate.startswith (\"+\")]\n",
    "    negatives = [candidate for candidate in sample if not candidate.startswith (\"+\")]\n",
    "    \n",
    "    return positives, negatives\n",
    "\n",
    "positives, negatives = readExamples (\"../data/names/annotated.txt\")\n",
    "sample = positives + negatives\n",
    "shuffle (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the constants!\n",
    "embeds_dim = 300\n",
    "hidden_dim = 300\n",
    "label_size = 2\n",
    "labels_map = {\"Name\":1, \"NonName\":0}\n",
    "iLabels_map = {1:\"Name\", 0:\"NonName\"}\n",
    "nEpochs = 10\n",
    "train_size, dev_size = 1000, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(candidate, iLabels_map[int (candidate in positives)]) \n",
    "        for candidate in sample \n",
    "        if candidate in early_vocab and candidate in later_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_embeddings (instance, emds):\n",
    "    \"\"\"Takes an instance and coverts into a vector\"\"\"\n",
    "    vec = torch.Tensor (emds.wv[instance])\n",
    "    return vec.view(1,-1)\n",
    "\n",
    "def make_target (label, mapping):\n",
    "    \"\"\"Takes a label and coverts it into a categorical variable\"\"\"\n",
    "    return torch.LongTensor([mapping[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200 381\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, test_data = data[:train_size], data[train_size:train_size+dev_size], data[train_size+dev_size:]\n",
    "print (len (train_data), len (dev_data), len (test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name removal using word embeddings approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (instances, emds, input_dim, hidden_dim, output_dim, iters, verbose=False):\n",
    "    global labels_map\n",
    "    model = classifier.TwoLayerNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    for epoch in range (iters):\n",
    "        training_loss = 0\n",
    "        for instance, label in instances:\n",
    "            # Step 1. Torch always accumulates the gradient by \n",
    "            # default, so make it zero if we want to backprop \n",
    "            # for every instance.\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Step2. Create the input and output\n",
    "            src = use_embeddings(instance, emds)\n",
    "            tgt = make_target (label, labels_map)\n",
    "            \n",
    "            # Step 3. Run the forward pass\n",
    "            log_probs = model.forward (src, F.relu)\n",
    "            \n",
    "            # Step 4. Calculate the loss; run the backward pass\n",
    "            # and make the gradient update.\n",
    "            loss = loss_function(log_probs, tgt)\n",
    "            loss.backward ()\n",
    "            optimizer.step ()\n",
    "        \n",
    "            training_loss += loss.item()\n",
    "            \n",
    "        if verbose:\n",
    "            print (\"Loss after epoch {0}:{1}\".format (epoch+1, training_loss/len(instances)))\n",
    "    return model\n",
    "\n",
    "def evaluate (model, instances, verbose=False):\n",
    "    global iLabels_map\n",
    "    predictions = list ()\n",
    "    labels = list ()\n",
    "    with torch.no_grad():\n",
    "        for instance, label in instances:\n",
    "            src = use_embeddings(instance, emds)\n",
    "            log_probs = model.forward (src, F.relu)\n",
    "            #prediction = F.softmax (log_probs, dim=1)\n",
    "            prediction = torch.argmax (log_probs, dim=1)[0].item()\n",
    "            predictions.append (iLabels_map[prediction])\n",
    "            labels.append (label)\n",
    "\n",
    "    acc = accuracy_score (labels, predictions)\n",
    "    auc = roc_auc_score (labels, predictions)\n",
    "    if verbose:\n",
    "        print (\"Accuracy after epoch {0}:{1}\".format (epoch+1, acc))\n",
    "        print (\"AUC after epoch {0}:{1}\".format (epoch+1, auc))\n",
    "            \n",
    "    return labels, predictions\n",
    "\n",
    "def train_and_evaluate (train_instances, test_instances, emds, input_dim, hidden_dim, output_dim, iters, verbose=False):\n",
    "    global labels_map\n",
    "    global iLabels_map\n",
    "    model = classifier.TwoLayerNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    for epoch in range (iters):\n",
    "        training_loss = 0\n",
    "        for instance, label in train_instances:\n",
    "            # Step 1. Torch always accumulates the gradient by \n",
    "            # default, so make it zero if we want to backprop \n",
    "            # for every instance.\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Step2. Create the input and output\n",
    "            src = use_embeddings(instance, emds)\n",
    "            tgt = make_target (label, labels_map)\n",
    "            \n",
    "            # Step 3. Run the forward pass\n",
    "            log_probs = model.forward (src, F.relu)\n",
    "            \n",
    "            # Step 4. Calculate the loss; run the backward pass\n",
    "            # and make the gradient update.\n",
    "            loss = loss_function(log_probs, tgt)\n",
    "            loss.backward ()\n",
    "            optimizer.step ()\n",
    "        \n",
    "            training_loss += loss.item()\n",
    "            \n",
    "        if verbose:\n",
    "            print (\"Loss after epoch {0}:{1}\".format (epoch+1, training_loss/len(train_instances)))\n",
    "            \n",
    "        test_predictions = list ()\n",
    "        test_labels = list ()\n",
    "        with torch.no_grad():\n",
    "            for instance, label in test_instances:\n",
    "                src = use_embeddings(instance, emds)\n",
    "                log_probs = model.forward (src, F.relu)\n",
    "                #prediction = F.softmax (log_probs, dim=1)\n",
    "                prediction = torch.argmax (log_probs, dim=1)[0].item()\n",
    "                test_predictions.append (prediction)\n",
    "                test_labels.append (labels_map[label])\n",
    "        acc = accuracy_score (test_labels, test_predictions)\n",
    "        auc = roc_auc_score (test_labels, test_predictions)\n",
    "        if verbose:\n",
    "            print (\"Accuracy after epoch {0}:{1}\".format (epoch+1, acc))\n",
    "            print (\"AUC after epoch {0}:{1}\".format (epoch+1, auc))\n",
    "            \n",
    "    return model, test_labels, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "twoLayerNet = classifier.TwoLayerNet(300, 300, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1:0.41055406073729195\n",
      "Accuracy after epoch 1:0.8923884514435696\n",
      "AUC after epoch 1:0.8985346826902805\n",
      "Loss after epoch 2:0.14180787801742553\n",
      "Accuracy after epoch 2:0.8740157480314961\n",
      "AUC after epoch 2:0.8393421884882984\n",
      "Loss after epoch 3:0.07544833421707153\n",
      "Accuracy after epoch 3:0.89501312335958\n",
      "AUC after epoch 3:0.8692283364958886\n",
      "Loss after epoch 4:0.04563492377599081\n",
      "Accuracy after epoch 4:0.8792650918635171\n",
      "AUC after epoch 4:0.8304870335230867\n",
      "Loss after epoch 5:0.02474735418955485\n",
      "Accuracy after epoch 5:0.8792650918635171\n",
      "AUC after epoch 5:0.821157495256167\n",
      "Loss after epoch 6:0.018852357069651285\n",
      "Accuracy after epoch 6:0.89501312335958\n",
      "AUC after epoch 6:0.844349567784103\n",
      "Loss after epoch 7:0.007827977339426676\n",
      "Accuracy after epoch 7:0.8766404199475065\n",
      "AUC after epoch 7:0.8038161501159603\n",
      "Loss after epoch 8:0.002113466262817383\n",
      "Accuracy after epoch 8:0.9028871391076115\n",
      "AUC after epoch 8:0.8746046805819102\n",
      "Loss after epoch 9:0.0011298807462056478\n",
      "Accuracy after epoch 9:0.905511811023622\n",
      "AUC after epoch 9:0.9012755639890364\n",
      "Loss after epoch 10:0.0014873973528544107\n",
      "Accuracy after epoch 10:0.9081364829396326\n",
      "AUC after epoch 10:0.9061775247733502\n"
     ]
    }
   ],
   "source": [
    "m1, _, _ = train_and_evaluate (train_data+dev_data, test_data, early_model, embeds_dim, hidden_dim, label_size, nEpochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1:0.49724227527777354\n",
      "Accuracy after epoch 1:0.863517060367454\n",
      "AUC after epoch 1:0.8694918827746152\n",
      "Loss after epoch 2:0.18090183138847352\n",
      "Accuracy after epoch 2:0.8451443569553806\n",
      "AUC after epoch 2:0.8569470799072317\n",
      "Loss after epoch 3:0.08653055707613627\n",
      "Accuracy after epoch 3:0.8792650918635171\n",
      "AUC after epoch 3:0.8553658022348725\n",
      "Loss after epoch 4:0.039121803442637125\n",
      "Accuracy after epoch 4:0.8740157480314961\n",
      "AUC after epoch 4:0.8517815728441914\n",
      "Loss after epoch 5:0.03561392863591512\n",
      "Accuracy after epoch 5:0.89501312335958\n",
      "AUC after epoch 5:0.8630086443179421\n",
      "Loss after epoch 6:0.00785568634668986\n",
      "Accuracy after epoch 6:0.8871391076115486\n",
      "AUC after epoch 6:0.8576323002319206\n",
      "Loss after epoch 7:0.002132910887400309\n",
      "Accuracy after epoch 7:0.8818897637795275\n",
      "AUC after epoch 7:0.8447185325743201\n",
      "Loss after epoch 8:0.000983403523763021\n",
      "Accuracy after epoch 8:0.8871391076115486\n",
      "AUC after epoch 8:0.8545224541429474\n",
      "Loss after epoch 9:0.0006149959564208984\n",
      "Accuracy after epoch 9:0.8871391076115486\n",
      "AUC after epoch 9:0.8545224541429474\n",
      "Loss after epoch 10:0.00044518470764160156\n",
      "Accuracy after epoch 10:0.8871391076115486\n",
      "AUC after epoch 10:0.8545224541429474\n"
     ]
    }
   ],
   "source": [
    "m2, _, _ = train_and_evaluate (train_data+dev_data, test_data, later_model, embeds_dim, hidden_dim, label_size, nEpochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n",
      "0.7322834645669292\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Baseline (most frequent class)\n",
    "print (\"Baseline\")\n",
    "baseline_predictions = [0 for instance, label in test_data]\n",
    "baseline_truth = [labels_map[label] for instance, label in test_data]\n",
    "print (accuracy_score (baseline_truth, baseline_predictions))\n",
    "print (roc_auc_score (baseline_truth, baseline_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply2vocab (model, emds):\n",
    "    global iLabels_map\n",
    "    predictions = dict()\n",
    "    with torch.no_grad():\n",
    "        for instance in emds.wv.vocab:\n",
    "            src = use_embeddings(instance, emds)\n",
    "            log_probs = model.forward (src, F.relu)\n",
    "            prediction = torch.argmax (log_probs, dim=1)[0].item()\n",
    "            predictions[instance] = iLabels_map[prediction]\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = apply2vocab (m1, early_model)\n",
    "V2 = apply2vocab (m2, later_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159639\n",
      "247005\n",
      "46512\n"
     ]
    }
   ],
   "source": [
    "neural_candidates = set (list (V1.keys())) & set (list (V2.keys()))\n",
    "neural_names = {candidate for candidate in neural_candidates if V1[candidate] == \"Name\" or V2[candidate] == \"Name\"}\n",
    "\n",
    "print (len(V1))\n",
    "print (len(V2))\n",
    "print (len (neural_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name removal using part of speech tagging heuristic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.5\n",
    "with open (os.path.join (STATS_DIR, \"ops.500K.early.nounprob\")) as fin:\n",
    "    P1 = {line.strip().split(\"\\t\")[0] for line in fin if float(line.strip().split(\"\\t\")[1]) >= thresh}\n",
    "with open (os.path.join (STATS_DIR, \"ops.500K.later.nounprob\")) as fin:\n",
    "    P2 = {line.strip().split(\"\\t\")[0] for line in fin if float(line.strip().split(\"\\t\")[1]) >= thresh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_names = P1 | P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789657\n"
     ]
    }
   ],
   "source": [
    "print (len (pos_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (os.path.join(STATS_DIR, \"names.neural\"), \"w\") as fout:\n",
    "    for name in neural_names:\n",
    "        fout.write (\"{0}\\n\".format (name))\n",
    "\n",
    "with open (os.path.join (STATS_DIR, \"names.tagging\"), \"w\") as fout:\n",
    "    for name in pos_names:\n",
    "        fout.write (\"{0}\\n\".format (name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, eliminate all words in the vocabulary that are either identified as names by the embedding approach or by part of speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34769"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len({v for v in common_vocab if v not in pos_names and v not in neural_names})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

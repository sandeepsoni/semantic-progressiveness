{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rosenfeld and Erk (R&E) model is a smart extension of SGNS which allows to learn diachronic embeddings i.e instead of one embedding per word like SGNS, learn one embedding per word for any given time.\n",
    "\n",
    "The basic idea of the R&E model is to replace the embeddings used in the SGNS objective with the output from a non-linear function. The non-linear function is implemented as a neural network which outputs the embedding for an input tuple of word and time.\n",
    "\n",
    "The advantages of the R&E model are:\n",
    "- the diachronic models are neural\n",
    "- time is treated as a continuous variable and used as an input (instead of initial preprocessing which divides the corpora in time chunks)\n",
    "- embeddings at any time can be obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is decomposed into two parts. The first part is the implementation of the basic SGNS model. In the second part, we extend the SGNS model to the R&E model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../modules\" not in sys.path:\n",
    "    sys.path.append (\"../modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchsgns import datastructures\n",
    "from pytorchsgns import dataloader\n",
    "from pytorchsgns import skipgram_models, difftime_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. SGNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgns_loss (pos_score, neg_score):\n",
    "    return -(pos_score + neg_score)\n",
    "\n",
    "class W2V (object):\n",
    "    def __init__ (self, textfile, \n",
    "                  min_count=5, \n",
    "                  embed_dims=32, \n",
    "                  negative=5, \n",
    "                  ns_exponent=0.75, \n",
    "                  window=5, \n",
    "                  learning_rate=0.001, \n",
    "                  subsampling=0.00001):\n",
    "        self.V, self.input_output_pairs = dataloader.RawReader.readDocs (textfile, min_count=min_count, ws=window)\n",
    "        self.V.subsample(sampling=subsampling, ns_exponent=ns_exponent)\n",
    "        self.negative = negative\n",
    "        self.model = skipgram_models.SkipGram (len(self.V.counts), embed_dims)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters (), lr=learning_rate) \n",
    "    \n",
    "    def negative_sampler (self, context_word):\n",
    "        vocab = [self.V.i2w[i] for i in range (len (self.V.i2w))]\n",
    "        p = [self.V.counts[self.V.i2w[i]] for i in range (len (self.V.i2w))]\n",
    "        sample = np.random.choice (vocab, size=self.negative, replace=True, p=p)\n",
    "        for w in sample:\n",
    "            if not w==context_word:\n",
    "                yield w\n",
    "    def lookup_tensor (self, w):\n",
    "        return torch.tensor([self.V.w2i[w]], dtype=torch.long)\n",
    "    \n",
    "    def train (self, nEpochs, verbose=False):\n",
    "        for epoch in range (nEpochs):\n",
    "            total_loss = 0\n",
    "            for input_word, output_word in self.input_output_pairs:\n",
    "                self.model.zero_grad ()\n",
    "                pos_score = self.model (self.lookup_tensor(input_word), self.lookup_tensor(output_word), sign=torch.ones(1))\n",
    "                neg_score = pos_score - pos_score #initialize to zero!maybe I'll lose the gradient so this hack. Revise.\n",
    "                for w in self.negative_sampler (output_word):\n",
    "                    neg_score += self.model (self.lookup_tensor(input_word), self.lookup_tensor(w), sign=-torch.ones(1))\n",
    "                loss = sgns_loss (pos_score, neg_score)\n",
    "                loss.backward ()\n",
    "                total_loss += loss.item()\n",
    "                self.optimizer.step()\n",
    "            if verbose: print (total_loss / len (self.input_output_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.77816900509995\n"
     ]
    }
   ],
   "source": [
    "w2v = W2V (\"mary_poppins.txt\")\n",
    "w2v.train(1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. DiffTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgns_loss (pos_score, neg_score):\n",
    "    return -(pos_score + neg_score)\n",
    "\n",
    "class W2VTime (object):\n",
    "    def __init__ (self, textfile, \n",
    "                  min_count=5, \n",
    "                  embed_dims=32, \n",
    "                  negative=5, \n",
    "                  ns_exponent=0.75, \n",
    "                  window=5, \n",
    "                  learning_rate=0.001, \n",
    "                  subsampling=0.00001):\n",
    "        self.V, self.input_output_pairs = dataloader.RawReader.readDocs (textfile, min_count=min_count, ws=window)\n",
    "        self.V.subsample(sampling=subsampling, ns_exponent=ns_exponent)\n",
    "        self.negative = negative\n",
    "        self.model = difftime_models.DiffTime ((10, 10), (10, 10), (10, 10), len(self.V.counts), embed_dims)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters (), lr=learning_rate) \n",
    "    \n",
    "    def negative_sampler (self, context_word):\n",
    "        vocab = [self.V.i2w[i] for i in range (len (self.V.i2w))]\n",
    "        p = [self.V.counts[self.V.i2w[i]] for i in range (len (self.V.i2w))]\n",
    "        sample = np.random.choice (vocab, size=self.negative, replace=True, p=p)\n",
    "        for w in sample:\n",
    "            if not w==context_word:\n",
    "                yield w\n",
    "    def lookup_tensor (self, w):\n",
    "        return torch.tensor([self.V.w2i[w]], dtype=torch.long)\n",
    "    \n",
    "    def train (self, nEpochs, verbose=False):\n",
    "        for epoch in range (nEpochs):\n",
    "            total_loss = 0\n",
    "            for input_word, output_word in self.input_output_pairs:\n",
    "                time=torch.Tensor ([[1.0]])\n",
    "                self.model.zero_grad ()\n",
    "                pos_score = self.model (self.lookup_tensor(input_word), self.lookup_tensor(output_word), time, sign=torch.ones(1))\n",
    "                neg_score = pos_score - pos_score #initialize to zero!maybe I'll lose the gradient so this hack. Revise.\n",
    "                for w in self.negative_sampler (output_word):\n",
    "                    neg_score += self.model (self.lookup_tensor(input_word), self.lookup_tensor(w), time, sign=-torch.ones(1))\n",
    "                loss = sgns_loss (pos_score, neg_score)\n",
    "                loss.backward ()\n",
    "                total_loss += loss.item()\n",
    "                self.optimizer.step()\n",
    "            if verbose: print (total_loss / len (self.input_output_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.020568932844624\n",
      "3.86865743110169\n",
      "3.8406590070898674\n"
     ]
    }
   ],
   "source": [
    "w2v = W2VTime (\"mary_poppins.txt\")\n",
    "w2v.train(3, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

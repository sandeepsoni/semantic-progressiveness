{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the function to score the document's semantic \"progressiveness\" with respect to a given linguistic feature (usually a word) that has been identified before to have semantically shifted in meaning. The scoring function assumes that two skipgram models have been trained. Please refer to the detailed derivation [in this note](https://github.gatech.edu/CompLingLab/semantic-lang-change/blob/master/notes/scoring.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scorer (object):\n",
    "    def __init__ (self, early_model, later_model, index, word):\n",
    "        \"\"\" constructor for scoring with respect to the given models and the word\n",
    "        \n",
    "        Args:\n",
    "            early_model (:obj: model): embeddings from early documents.\n",
    "            later_model (:obj: model): embeddings from later documents.\n",
    "            index (:obj: tuple): an index to map words to position in the matrix and vice versa.\n",
    "            word (:obj: str): the word\n",
    "        \"\"\"\n",
    "        self.m_early = early_model\n",
    "        self.m_later = later_model\n",
    "        self.w2i, self.i2w = index\n",
    "        self.word = word\n",
    "        \n",
    "        self.sim_early = np.dot (self.m_early.C, self.m_early.W[self.w2i[word]])\n",
    "        self.sim_later = np.dot (self.m_later.C, self.m_later.W[self.w2i[word]])\n",
    "        z_early = scipy.special.logsumexp (self.sim_early)\n",
    "        z_later = scipy.special.logsumexp (self.sim_later)\n",
    "        \n",
    "        self.zdiff = z_later - z_early\n",
    "        \n",
    "    def score (self, doc, window_size=10):\n",
    "        \"\"\"score a given document with respect to the word\n",
    "        \n",
    "        Args:\n",
    "            doc (:obj: list): document as a list of tokens.\n",
    "            window_size (int, optional): the size of the context window (default=10)\n",
    "        \n",
    "        Returns:\n",
    "            float: document score wrt `self.word`, `self.early_model` and `self.later_model`.\n",
    "            \n",
    "        Todo:\n",
    "            1. test if the scoring function is correct using several test cases.\n",
    "        \"\"\"\n",
    "        def make_contexts_bow (doc, word, w2i, k=10):\n",
    "            \"\"\"makes a bow vector of contexts around the target word\n",
    "            \n",
    "            Args:\n",
    "                doc (:obj: list): document as a list of tokens.\n",
    "                word(:obj: str): the target word\n",
    "                w2i(:obj: dict): maps any word to a position.\n",
    "                k(int, optional): the window size around `word` (default=10)\n",
    "            \"\"\"\n",
    "            last_token_index = len(doc) - 1\n",
    "            word_positions = [i for i, token in enumerate (doc) if token == word]\n",
    "            spans = [(max(0, pos-k), min(last_token_index, pos+k)) for pos in word_positions]\n",
    "            bow = np.zeros (len(w2i))\n",
    "            \n",
    "            for i, span in enumerate (spans):\n",
    "                start, end = span\n",
    "                for pos in range (start, end+1):\n",
    "                    if not pos == word_positions[i]:\n",
    "                        bow[w2i[doc[pos]]] += 1\n",
    "                        \n",
    "            return bow\n",
    "        \n",
    "        contexts_bow = make_contexts_bow (doc, self.word, self.w2i, k=window_size)\n",
    "        print (contexts_bow)\n",
    "        cooccurrence_factor = np.dot (contexts_bow, self.sim_later - self.sim_early)\n",
    "        normalization_factor = sum(contexts_bow) * self.zdiff\n",
    "        score = cooccurrence_factor - normalization_factor\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model (object):\n",
    "    def __init__ (self, C, W):\n",
    "        self.C = C\n",
    "        self.W = W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are some examples to sanity check the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 1: Assume all embeddings -- input and output -- are equal. Let this vector be $\\mathbf{p}$. If $\\mathbf{p}=q\\mathbf{1}$, then the answer should be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=10\n",
    "vocab = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"i\", \"j\", \"k\", \"l\"]\n",
    "dims = 10\n",
    "C_early = C_later = W_early = W_later = q*np.ones((len(vocab),dims))\n",
    "w2i, i2w = {w:i for i,w in enumerate (vocab)},{i:w for i,w in enumerate (vocab)}\n",
    "doc = list (\"fadekelfi\")\n",
    "k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.] [1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.]\n",
      "1002.302585092994 1002.302585092994\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "m_early = Model (C_early, W_early)\n",
    "m_later = Model (C_later, W_later)\n",
    "scorer = Scorer (m_early, m_later, (w2i, i2w), \"k\")\n",
    "print (scorer.score(doc, window_size=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: Assume that the output embeddings are same and the input embeddings are scales of each other the score is still 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=1\n",
    "vocab = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"i\", \"j\", \"k\", \"l\"]\n",
    "dims = 10\n",
    "C_early = C_later = q*np.ones((len(vocab),dims))\n",
    "W_early = q*np.random.randn(len(vocab), dims)\n",
    "W_later = 10*W_early\n",
    "w2i, i2w = {w:i for i,w in enumerate (vocab)},{i:w for i,w in enumerate (vocab)}\n",
    "doc = list (\"fadekelfi\")\n",
    "k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "m_early = Model (C_early, W_early)\n",
    "m_later = Model (C_later, W_later)\n",
    "scorer = Scorer (m_early, m_later, (w2i, i2w), \"k\")\n",
    "print (scorer.score(doc, window_size=k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 3: Assume randomly initialized but equal output embeddings and input embeddings as scales of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"i\", \"j\", \"k\", \"l\"]\n",
    "dims = 10\n",
    "C_early = C_later = np.random.randn(len(vocab), dims)\n",
    "W_early = np.random.randn(len(vocab), dims)\n",
    "W_later = 2*W_early\n",
    "w2i, i2w = {w:i for i,w in enumerate (vocab)},{i:w for i,w in enumerate (vocab)}\n",
    "doc = list (\"fadekelfi\")\n",
    "k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. 1. 2. 1. 0. 0. 2. 1.]\n",
      "-29.977487567077777\n"
     ]
    }
   ],
   "source": [
    "m_early = Model (C_early, W_early)\n",
    "m_later = Model (C_later, W_later)\n",
    "scorer = Scorer (m_early, m_later, (w2i, i2w), \"e\")\n",
    "print (scorer.score(doc, window_size=k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-29.977487567077777\n"
     ]
    }
   ],
   "source": [
    "W_later = 3*W_early\n",
    "m_early = Model (C_early, W_early)\n",
    "m_later = Model (C_later, W_later)\n",
    "scorer = Scorer (m_early, m_later, (w2i, i2w), \"e\")\n",
    "print (scorer.score(doc, window_size=k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

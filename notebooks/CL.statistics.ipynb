{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson\n",
    "import os\n",
    "import sys\n",
    "import heapq\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"../\" not in sys.path:\n",
    "    sys.path.append (\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/hg191/corpora/legaldata/data/\"\n",
    "FILES_DIR = os.path.join (DATA_DIR, \"files\")\n",
    "STATS_DIR = os.path.join (DATA_DIR, \"stats\")\n",
    "JURS_FILE = os.path.join (DATA_DIR, \"jurs.names\")\n",
    "CITES_FILE = os.path.join (DATA_DIR, \"cites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs (STATS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (JURS_FILE) as fin:\n",
    "    jurs = [line.strip() for line in fin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_iterator (filename):\n",
    "    with open (filename) as fin:\n",
    "        for line in fin:\n",
    "            yield ujson.loads (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeAndWrite (jurs):\n",
    "    iterables = [json_iterator (os.path.join (FILES_DIR, jur + constants.JSONL_EXT)) for jur in jurs]\n",
    "    \n",
    "    OPS_FILE = os.path.join (STATS_DIR, \"ops.list\")\n",
    "    DATES_FILE = os.path.join (STATS_DIR, \"ops.dates\")\n",
    "    STATUS_FILE = os.path.join (STATS_DIR, \"ops.pubs\")\n",
    "    COURTS_FILE = os.path.join (STATS_DIR, \"ops.courts\")\n",
    "    TYPES_FILE = os.path.join (STATS_DIR, \"ops.types\")\n",
    "    \n",
    "    with open (OPS_FILE, \"w\") as ops_out, open (DATES_FILE, \"w\") as dates_out, open (STATUS_FILE, \"w\") as status_out, open(COURTS_FILE, \"w\") as courts_out, open(TYPES_FILE, \"w\") as types_out:\n",
    "        merged = heapq.merge (*iterables, key=lambda x:x[\"date\"])\n",
    "        for js in merged:\n",
    "            ops_out.write (\"{0}\\n\".format (js[\"opid\"]))\n",
    "            dates_out.write (\"{0},{1}\\n\".format (js[\"opid\"], js[\"date\"]))\n",
    "            status_out.write (\"{0},{1}\\n\".format (js[\"opid\"], js[\"isapub\"]))\n",
    "            courts_out.write (\"{0},{1}\\n\".format (js[\"opid\"], js[\"court\"]))\n",
    "            types_out.write (\"{0},{1}\\n\".format (js[\"opid\"], js[\"type\"]))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. General statistics\n",
    "\n",
    "- date of the document\n",
    "- court of the document\n",
    "- precendential status\n",
    "- if opinion has a dissent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeAndWrite(jurs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCitationNetAsDicts (filename):\n",
    "    indict = defaultdict (int)\n",
    "    outdict = defaultdict (int)\n",
    "    \n",
    "    df = pd.read_csv (filename, sep=\",\")\n",
    "    \n",
    "    froms = df[\"citing_opinion_id\"].values\n",
    "    tos = df[\"cited_opinion_id\"].values\n",
    "    \n",
    "    for i in range (len(froms)):\n",
    "        outdict[froms[i]] += 1\n",
    "        indict[tos[i]] += 1\n",
    "    \n",
    "    return indict, outdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPS_FILE = os.path.join (STATS_DIR, \"ops.list\")\n",
    "ind_dict, outd_dict = readCitationNetAsDicts(CITES_FILE)\n",
    "\n",
    "IND_FILE = os.path.join (STATS_DIR, \"ops.ind\")\n",
    "OUTD_FILE = os.path.join (STATS_DIR, \"ops.outd\")\n",
    "\n",
    "with open (OPS_FILE) as fin, open(IND_FILE, \"w\") as indout, open (OUTD_FILE, \"w\") as outdout:\n",
    "    for line in fin:\n",
    "        op = int(line.strip())\n",
    "        indout.write (\"{0},{1}\\n\".format (op, ind_dict[op]))\n",
    "        outdout.write (\"{0},{1}\\n\".format (op, outd_dict[op]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. Document statistics\n",
    "\n",
    "- number of pages\n",
    "- number of statutes mentioned\n",
    "- document length (number of unique types and number of tokens)\n",
    "- bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeAndWriteDocs (jurs):\n",
    "    filenames = [os.path.join (FILES_DIR, jur + \".tokenized\" + constants.JSONL_EXT) for jur in jurs]\n",
    "    iterables = [json_iterator (filename) for filename in filenames]\n",
    "    \n",
    "    #PAGES_FILE = os.path.join (STATS_DIR, \"ops.pages\")\n",
    "    #LENGTH_FILES = [os.path.join (STATS_DIR, x) for x in [\"ops.nuniqs\", \"ops.ntokens\"]]\n",
    "    #DOCS_FILE = os.path.join (STATS_DIR, \"ops.docs\")\n",
    "    TEXTS_FILE = os.path.join (STATS_DIR, \"ops.texts\")\n",
    "    \n",
    "    #with open (PAGES_FILE, \"w\") as p_out, open (LENGTH_FILES[0], \"w\") as nuniqs_out, open (LENGTH_FILES[1], \"w\") as ntokens_out, open (DOCS_FILE, \"w\") as docs_out:\n",
    "    with open (TEXTS_FILE, \"w\") as texts_out:\n",
    "        merged = heapq.merge (*iterables, key=lambda x:x[\"date\"])\n",
    "        for js in merged:\n",
    "            texts_out.write (\"{0}\\n\".format(js[\"text\"].replace(\"\\r\", \"\").replace(\"\\n\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeAndWriteDocs (jurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. create sub opinions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subopscreation (jurs):\n",
    "    filenames = [os.path.join (FILES_DIR, jur + \".tokenized\" + constants.JSONL_EXT) for jur in jurs]\n",
    "    iterables = [json_iterator (filename) for filename in filenames]\n",
    "    \n",
    "    SUBOPS_FILE = os.path.join (STATS_DIR, \"ops.subops\")\n",
    "    \n",
    "    with open (SUBOPS_FILE, \"w\") as fout:\n",
    "        merged = heapq.merge (*iterables, key=lambda x:x[\"date\"])\n",
    "        for js in merged:\n",
    "            opid = js[\"opid\"]\n",
    "            subops = \"$\".join ([str(subop) for subop in js[\"subops\"]])\n",
    "            fout.write (\"{0},{1}\\n\".format (opid, subops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subopscreation(jurs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. Network statistics\n",
    "\n",
    "- outdegree\n",
    "- indegree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCitationNetAsDicts (filename):\n",
    "    indict = defaultdict (int)\n",
    "    outdict = defaultdict (int)\n",
    "    \n",
    "    df = pd.read_csv (filename, sep=\",\")\n",
    "    \n",
    "    froms = df[\"citing_opinion_id\"].values\n",
    "    tos = df[\"cited_opinion_id\"].values\n",
    "    \n",
    "    for i in range (len(froms)):\n",
    "        outdict[froms[i]] += 1\n",
    "        indict[tos[i]] += 1\n",
    "    \n",
    "    return indict, outdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read citation network\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-865f24d27bc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msubops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutd_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mindout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"{0},{1}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "OPS_FILE = os.path.join (STATS_DIR, \"ops.subops\")\n",
    "ind_dict, outd_dict = readCitationNetAsDicts(CITES_FILE)\n",
    "print (\"Read citation network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "IND_FILE = os.path.join (STATS_DIR, \"ops.ind\")\n",
    "OUTD_FILE = os.path.join (STATS_DIR, \"ops.outd\")\n",
    "\n",
    "ind_exceptions = 0\n",
    "outd_exceptions = 0\n",
    "pointerin_dict = {}\n",
    "pointerout_dict = {}\n",
    "\n",
    "with open (OPS_FILE) as fin, open(IND_FILE, \"w\") as indout, open (OUTD_FILE, \"w\") as outdout:\n",
    "    for line in fin:\n",
    "        parts = line.strip().split(\",\")\n",
    "        op = int(parts[0])\n",
    "        subops = list (map(int, parts[1].split(\"$\")))\n",
    "        if len(subops) > 1:\n",
    "            if op not in pointerin_dict:\n",
    "                pointerin_dict[op] = 0\n",
    "            if op not in pointerout_dict:\n",
    "                pointerout_dict[op] = 0\n",
    "            \n",
    "            in_index = pointerin_dict[op]\n",
    "            out_index = pointerout_dict[op]\n",
    "        else:\n",
    "            in_index = out_index = 0\n",
    "        indout.write (\"{0},{1}\\n\".format (op, ind_dict[subops[in_index]]))\n",
    "        outdout.write (\"{0},{1}\\n\".format (op, outd_dict[subops[out_index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52953\n",
      "63416\n"
     ]
    }
   ],
   "source": [
    "print (ind_exceptions)\n",
    "print (outd_exceptions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

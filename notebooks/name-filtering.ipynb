{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two heuristics are used to identify names:\n",
    "\n",
    "* Word classification: Use word embeddings learned from the two skipgram models to train separate neural classifiers. A word is considered as a name if both the classifiers classify it as a name.\n",
    "* PoS tagging: A word is considered as a name if more than 90% times it is tagged as NNP or NNPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Name Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"../modules/\" not in sys.path: sys.path.append (\"../modules/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from propernames.classifier import TwoLayerNet\n",
    "from semshift.embeddings import TrainedModel    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f66a29bd110>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = TrainedModel (\"/hg191/corpora/legaldata/sc-docs/0.model\")\n",
    "M2 = TrainedModel (\"/hg191/corpora/legaldata/sc-docs/7.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readExamples (filename):\n",
    "    with open (filename) as fin:\n",
    "        sample = [line.strip() for line in fin]\n",
    "    positives = [candidate.strip(\"+\") for candidate in sample if candidate.startswith (\"+\")]\n",
    "    negatives = [candidate for candidate in sample if not candidate.startswith (\"+\")]\n",
    "    \n",
    "    return positives, negatives\n",
    "\n",
    "positives, negatives = readExamples (\"../data/names/annotated.txt\")\n",
    "sample = positives + negatives\n",
    "shuffle (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the constants!\n",
    "embeds_dim = 100\n",
    "hidden_dim = 100\n",
    "label_size = 2\n",
    "labels_map = {\"Name\":1, \"NonName\":0}\n",
    "iLabels_map = {1:\"Name\", 0:\"NonName\"}\n",
    "nEpochs = 15\n",
    "train_size, dev_size = 1000, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(candidate, iLabels_map[int (candidate in positives)]) \n",
    "        for candidate in sample \n",
    "        if candidate in M1.m.wv.vocab and candidate in M2.m.wv.vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_embeddings (instance, emds):\n",
    "    \"\"\"Takes an instance and coverts into a vector\"\"\"\n",
    "    vec = torch.Tensor (emds.m.wv[instance])\n",
    "    return vec.view(1,-1)\n",
    "\n",
    "def make_target (label, mapping):\n",
    "    \"\"\"Takes a label and coverts it into a categorical variable\"\"\"\n",
    "    return torch.LongTensor([mapping[label]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 200 389\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, test_data = data[:train_size], data[train_size:train_size+dev_size], data[train_size+dev_size:]\n",
    "print (len (train_data), len (dev_data), len (test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (instances, emds, input_dim, hidden_dim, output_dim, iters, verbose=False):\n",
    "    global labels_map\n",
    "    model = TwoLayerNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    for epoch in range (iters):\n",
    "        training_loss = 0\n",
    "        for instance, label in instances:\n",
    "            # Step 1. Torch always accumulates the gradient by \n",
    "            # default, so make it zero if we want to backprop \n",
    "            # for every instance.\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Step2. Create the input and output\n",
    "            src = use_embeddings(instance, emds)\n",
    "            tgt = make_target (label, labels_map)\n",
    "            \n",
    "            # Step 3. Run the forward pass\n",
    "            log_probs = model.forward (src, F.relu)\n",
    "            \n",
    "            # Step 4. Calculate the loss; run the backward pass\n",
    "            # and make the gradient update.\n",
    "            loss = loss_function(log_probs, tgt)\n",
    "            loss.backward ()\n",
    "            optimizer.step ()\n",
    "        \n",
    "            training_loss += loss.item()\n",
    "            \n",
    "        if verbose:\n",
    "            print (\"Loss after epoch {0}:{1}\".format (epoch+1, training_loss/len(instances)))\n",
    "    return model\n",
    "\n",
    "def evaluate (model, instances, verbose=False):\n",
    "    global iLabels_map\n",
    "    predictions = list ()\n",
    "    labels = list ()\n",
    "    with torch.no_grad():\n",
    "        for instance, label in instances:\n",
    "            src = use_embeddings(instance, emds)\n",
    "            log_probs = model.forward (src, F.relu)\n",
    "            #prediction = F.softmax (log_probs, dim=1)\n",
    "            prediction = torch.argmax (log_probs, dim=1)[0].item()\n",
    "            predictions.append (iLabels_map[prediction])\n",
    "            labels.append (label)\n",
    "\n",
    "    acc = accuracy_score (labels, predictions)\n",
    "    auc = roc_auc_score (labels, predictions)\n",
    "    if verbose:\n",
    "        print (\"Accuracy after epoch {0}:{1}\".format (epoch+1, acc))\n",
    "        print (\"AUC after epoch {0}:{1}\".format (epoch+1, auc))\n",
    "            \n",
    "    return labels, predictions\n",
    "\n",
    "def train_and_evaluate (train_instances, test_instances, emds, input_dim, hidden_dim, output_dim, iters, verbose=False):\n",
    "    global labels_map\n",
    "    global iLabels_map\n",
    "    model = TwoLayerNet(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    for epoch in range (iters):\n",
    "        training_loss = 0\n",
    "        for instance, label in train_instances:\n",
    "            # Step 1. Torch always accumulates the gradient by \n",
    "            # default, so make it zero if we want to backprop \n",
    "            # for every instance.\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Step2. Create the input and output\n",
    "            src = use_embeddings(instance, emds)\n",
    "            tgt = make_target (label, labels_map)\n",
    "            \n",
    "            # Step 3. Run the forward pass\n",
    "            log_probs = model.forward (src, F.relu)\n",
    "            \n",
    "            # Step 4. Calculate the loss; run the backward pass\n",
    "            # and make the gradient update.\n",
    "            loss = loss_function(log_probs, tgt)\n",
    "            loss.backward ()\n",
    "            optimizer.step ()\n",
    "        \n",
    "            training_loss += loss.item()\n",
    "            \n",
    "        if verbose:\n",
    "            print (\"Loss after epoch {0}:{1}\".format (epoch+1, training_loss/len(train_instances)))\n",
    "            \n",
    "        test_predictions = list ()\n",
    "        test_labels = list ()\n",
    "        with torch.no_grad():\n",
    "            for instance, label in test_instances:\n",
    "                src = use_embeddings(instance, emds)\n",
    "                log_probs = model.forward (src, F.relu)\n",
    "                #prediction = F.softmax (log_probs, dim=1)\n",
    "                prediction = torch.argmax (log_probs, dim=1)[0].item()\n",
    "                test_predictions.append (prediction)\n",
    "                test_labels.append (labels_map[label])\n",
    "        acc = accuracy_score (test_labels, test_predictions)\n",
    "        auc = roc_auc_score (test_labels, test_predictions)\n",
    "        if verbose:\n",
    "            print (\"Accuracy after epoch {0}:{1}\".format (epoch+1, acc))\n",
    "            print (\"AUC after epoch {0}:{1}\".format (epoch+1, auc))\n",
    "            \n",
    "    return model, test_labels, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1:0.4165320779879888\n",
      "Accuracy after epoch 1:0.922879177377892\n",
      "AUC after epoch 1:0.9324698598892147\n",
      "Loss after epoch 2:0.23977441946665445\n",
      "Accuracy after epoch 2:0.922879177377892\n",
      "AUC after epoch 2:0.9352231997393288\n",
      "Loss after epoch 3:0.217659543355306\n",
      "Accuracy after epoch 3:0.9280205655526992\n",
      "AUC after epoch 3:0.9415607689801239\n",
      "Loss after epoch 4:0.20182730952898661\n",
      "Accuracy after epoch 4:0.9280205655526992\n",
      "AUC after epoch 4:0.9415607689801239\n",
      "Loss after epoch 5:0.19010080774625143\n",
      "Accuracy after epoch 5:0.9280205655526992\n",
      "AUC after epoch 5:0.9415607689801239\n",
      "Loss after epoch 6:0.17712292671203614\n",
      "Accuracy after epoch 6:0.9280205655526992\n",
      "AUC after epoch 6:0.9388074291300098\n",
      "Loss after epoch 7:0.17312798897425333\n",
      "Accuracy after epoch 7:0.9254498714652957\n",
      "AUC after epoch 7:0.9370153144346693\n",
      "Loss after epoch 8:0.1615299435456594\n",
      "Accuracy after epoch 8:0.9254498714652957\n",
      "AUC after epoch 8:0.9370153144346693\n",
      "Loss after epoch 9:0.14836889266967773\n",
      "Accuracy after epoch 9:0.9151670951156813\n",
      "AUC after epoch 9:0.9298468556533073\n",
      "Loss after epoch 10:0.13701395789782206\n",
      "Accuracy after epoch 10:0.9203084832904884\n",
      "AUC after epoch 10:0.9306777451938743\n",
      "Loss after epoch 11:0.12346814314524333\n",
      "Accuracy after epoch 11:0.9177377892030848\n",
      "AUC after epoch 11:0.9288856304985337\n",
      "Loss after epoch 12:0.11102273265520732\n",
      "Accuracy after epoch 12:0.9125964010282777\n",
      "AUC after epoch 12:0.9253014011078528\n",
      "Loss after epoch 13:0.10032658338546753\n",
      "Accuracy after epoch 13:0.9125964010282777\n",
      "AUC after epoch 13:0.9253014011078528\n",
      "Loss after epoch 14:0.08679763873418173\n",
      "Accuracy after epoch 14:0.8997429305912596\n",
      "AUC after epoch 14:0.9108341479309222\n",
      "Loss after epoch 15:0.07460019111633301\n",
      "Accuracy after epoch 15:0.910025706940874\n",
      "AUC after epoch 15:0.9152492668621701\n"
     ]
    }
   ],
   "source": [
    "model1, _, _ = train_and_evaluate (train_data+dev_data, test_data, M1, embeds_dim, hidden_dim, label_size, nEpochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1:0.427378475467364\n",
      "Accuracy after epoch 1:0.9151670951156813\n",
      "AUC after epoch 1:0.9078201368523949\n",
      "Loss after epoch 2:0.23741979161898294\n",
      "Accuracy after epoch 2:0.9177377892030848\n",
      "AUC after epoch 2:0.9178722710980776\n",
      "Loss after epoch 3:0.21525579690933228\n",
      "Accuracy after epoch 3:0.9125964010282777\n",
      "AUC after epoch 3:0.9142880417073966\n",
      "Loss after epoch 4:0.19917047142982483\n",
      "Accuracy after epoch 4:0.910025706940874\n",
      "AUC after epoch 4:0.9152492668621701\n",
      "Loss after epoch 5:0.19035330096880596\n",
      "Accuracy after epoch 5:0.9023136246786633\n",
      "AUC after epoch 5:0.9098729227761485\n",
      "Loss after epoch 6:0.18008546352386476\n",
      "Accuracy after epoch 6:0.8946015424164524\n",
      "AUC after epoch 6:0.9072499185402412\n",
      "Loss after epoch 7:0.1696396239598592\n",
      "Accuracy after epoch 7:0.8946015424164524\n",
      "AUC after epoch 7:0.904496578690127\n",
      "Loss after epoch 8:0.15700855334599814\n",
      "Accuracy after epoch 8:0.8920308483290489\n",
      "AUC after epoch 8:0.9082111436950147\n",
      "Loss after epoch 9:0.14716411550839742\n",
      "Accuracy after epoch 9:0.8920308483290489\n",
      "AUC after epoch 9:0.9082111436950147\n",
      "Loss after epoch 10:0.1359798792997996\n",
      "Accuracy after epoch 10:0.8868894601542416\n",
      "AUC after epoch 10:0.8963668947539916\n",
      "Loss after epoch 11:0.12120806256930033\n",
      "Accuracy after epoch 11:0.884318766066838\n",
      "AUC after epoch 11:0.894574780058651\n",
      "Loss after epoch 12:0.1057266374429067\n",
      "Accuracy after epoch 12:0.8688946015424165\n",
      "AUC after epoch 12:0.881068752036494\n",
      "Loss after epoch 13:0.09611483891805013\n",
      "Accuracy after epoch 13:0.87146529562982\n",
      "AUC after epoch 13:0.8773541870316064\n",
      "Loss after epoch 14:0.0818139370282491\n",
      "Accuracy after epoch 14:0.884318766066838\n",
      "AUC after epoch 14:0.8780547409579668\n",
      "Loss after epoch 15:0.06828797976175945\n",
      "Accuracy after epoch 15:0.884318766066838\n",
      "AUC after epoch 15:0.8780547409579668\n"
     ]
    }
   ],
   "source": [
    "model2, _, _ = train_and_evaluate (train_data+dev_data, test_data, M2, embeds_dim, hidden_dim, label_size, nEpochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n",
      "0.7172236503856041\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Baseline (most frequent class)\n",
    "print (\"Baseline\")\n",
    "baseline_predictions = [0 for instance, label in test_data]\n",
    "baseline_truth = [labels_map[label] for instance, label in test_data]\n",
    "print (accuracy_score (baseline_truth, baseline_predictions))\n",
    "print (roc_auc_score (baseline_truth, baseline_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply2vocab (model, emds):\n",
    "    global iLabels_map\n",
    "    predictions = dict()\n",
    "    with torch.no_grad():\n",
    "        for instance in emds.m.wv.vocab:\n",
    "            src = use_embeddings(instance, emds)\n",
    "            log_probs = model.forward (src, F.relu)\n",
    "            prediction = torch.argmax (log_probs, dim=1)[0].item()\n",
    "            predictions[instance] = iLabels_map[prediction]\n",
    "            \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = apply2vocab (model1, M1)\n",
    "V2 = apply2vocab (model2, M2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158629\n",
      "190451\n",
      "34559\n"
     ]
    }
   ],
   "source": [
    "neural_candidates = set (list (V1.keys())) & set (list (V2.keys()))\n",
    "neural_names = {candidate for candidate in neural_candidates if V1[candidate] == \"Name\" and V2[candidate] == \"Name\"}\n",
    "\n",
    "print (len(V1))\n",
    "print (len(V2))\n",
    "print (len (neural_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51522\n"
     ]
    }
   ],
   "source": [
    "print (len ({c for c in neural_candidates if V1[c] == \"Name\" or V2[c] == \"Name\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.90\n",
    "with open (\"/hg191/corpora/legaldata/sc-docs/0.nounprob\") as fin:\n",
    "    P1 = {line.strip().split(\"\\t\")[0] for line in fin if float(line.strip().split(\"\\t\")[1]) >= thresh}\n",
    "with open (\"/hg191/corpora/legaldata/sc-docs/7.nounprob\") as fin:\n",
    "    P2 = {line.strip().split(\"\\t\")[0] for line in fin if float(line.strip().split(\"\\t\")[1]) >= thresh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_names = P1 & P2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write output of both the methods to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"/hg191/corpora/legaldata/sc-docs/names.neural\", \"w\") as fout:\n",
    "    for name in neural_names:\n",
    "        fout.write (\"{0}\\n\".format (name))\n",
    "\n",
    "with open (\"/hg191/corpora/legaldata/sc-docs/names.tagging\", \"w\") as fout:\n",
    "    for name in pos_names:\n",
    "        fout.write (\"{0}\\n\".format (name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
